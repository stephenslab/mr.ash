---
title: "Introduction to Mr.ASH"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Introduction to Mr.ASH}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

Here we go through a simple example of using Multiple Regression Adaptive Shrinkage, < maybe insert paper link here? >

```{r setup}
library(mr.ash)
```


## Step 1: Data Simulation

We simulate a regression data matrix X with n = 400 samples and p = 2000 variables, and a normal response vector y: 

```{r}
set.seed(1)
n <- 400
p <- 1000
pve <- 0.5
s <- 10

## I feel like I shouldn't need to sources these two files, but it keeps saying function
## not found when I knit/build after loading even tho I can use it freely in commandline
source("../R/misc2.R")
source("../R/datasim.R")
sim_data <- simulate_data(n, p, pve, s)
```


## Step 2: Fitting Mr.ASH

This is the most basic usage, using the default settings:

```{r, results='hide'}
fit <- mr_ash(sim_data$X,sim_data$y,
              control = list(max.iter = 500,convtol = 1e-12),
              verbose = "detailed")

# iter                elbo ||b-b'||   sigma2 w>0
#    1 +2.405642681821e+03 2.96e+00 7.05e-01  20
#    2 +1.327075779103e+03 3.36e-01 5.71e-01  20
#    3 +9.868513871648e+02 2.00e-01 5.93e-01  20
#    4 +8.541453355042e+02 1.37e-01 6.34e-01  20
#    5 +7.866237345676e+02 1.09e-01 6.73e-01  20
#    6 +7.471872913125e+02 8.65e-02 7.04e-01  17
#    7 +7.228338002780e+02 6.18e-02 7.31e-01  13
#    8 +7.069318149041e+02 4.17e-02 7.53e-01  11
#    9 +6.957980471234e+02 2.99e-02 7.72e-01  10
#   10 +6.876027709319e+02 2.20e-02 7.88e-01   9
#  ........
#  490 +6.456370631675e+02 1.15e-07 9.82e-01   3
#  491 +6.456370592689e+02 1.13e-07 9.82e-01   3
#  492 +6.456370554377e+02 1.11e-07 9.82e-01   3
#  493 +6.456370516727e+02 1.09e-07 9.82e-01   3
#  494 +6.456370479728e+02 1.08e-07 9.82e-01   3
#  495 +6.456370443367e+02 1.06e-07 9.82e-01   3
#  496 +6.456370407633e+02 1.04e-07 9.82e-01   3
#  497 +6.456370372514e+02 1.03e-07 9.82e-01   3
#  498 +6.456370338000e+02 1.01e-07 9.82e-01   3
#  499 +6.456370304080e+02 9.95e-08 9.82e-01   3
#  500 +6.456370270742e+02 9.80e-08 9.82e-01   3
# Mr.ASH terminated at iteration 500.
```

The default model is a mixture with 20 components, with prior mixture component variances in increasing order, starting at zero and ending at roughly 1: 

```{r}
fit$sa2
```

For each iteration, we calculate the Evidence Lower Bound (ELBO) until the maximum iterations specified is reached (in the case demonstrated here), or when convergence criterion is met. We can observe that the estimated $\beta$ is a good reflection of the 

```{r}
elbo <- fit$progress$elbo
elbo <- elbo - min(elbo) + 1e-6
plot(fit$progress$iter, elbo, type = "l",log = "y",lwd = 2,col = "dodgerblue")
plot(sim_data$beta,coef(fit)[-1],pch = 20,col = "darkblue",xlab = "true",ylab = "estimate")
abline(a = 0,b = 1,col = "skyblue",lty = "dotted")
```

## Step 3: Extract Posterior Summaries

```{r}
## Another *source* issue
source("../R/misc.R")
posterior <- get_posterior_summary(fit)
str(posterior)

head(posterior$m) 
head(posterior$s2)
head(posterior$lfsr)
```

Posterior means and variances are $p \times K$ matrices, while posterior local false discovery rate (lfsr) is a vector of length $p$. 

## Session Information: 

```{r}
print(sessionInfo())
```




