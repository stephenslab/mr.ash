% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/fit.R, R/init.R
\name{fit_mr_ash}
\alias{fit_mr_ash}
\alias{mr_ash_control_default}
\alias{init_mr_ash}
\title{Multiple Regression with Adaptive Shrinkage}
\usage{
fit_mr_ash(
  X,
  y,
  fit0 = init_mr_ash(X, y),
  standardize = FALSE,
  intercept = TRUE,
  control = list(),
  verbose = c("progress", "detailed", "none")
)

mr_ash_control_default()

init_mr_ash(
  X,
  y,
  b,
  prior.sd,
  prior.weights,
  resid.sd,
  init.method = c("glmnet", "null"),
  s = "lambda.1se",
  ...
)
}
\arguments{
\item{X}{The data matrix, a numeric matrix of dimension n x p; each
column is a single predictor, and each row is an observation
vector. Here, n is the number of samples and p is the number of
predictors.}

\item{y}{The observed outcomes, a numeric vector of length n.}

\item{fit0}{Initialized \code{mr.ash} object resulting from a call to
\code{init_mr_ash}.}

\item{standardize}{The logical flag for standardization of the
columns of X variable, prior to the model fitting. The coefficients
are always returned on the original scale.}

\item{intercept}{When \code{intercept = TRUE}, an intercept is
included in the regression model.}

\item{control}{A list of parameters controlling the behaviour of
the optimization algorithm. See \sQuote{Details}.}

\item{verbose}{When \code{verbose = "detailed"}, detailed
information about the algorithm's progress is printed to the
console at each iteration; when \code{verbose = "progressbar"}, a
plus (\dQuote{+}) is printed for each outer-loop iteration; and
when \code{verbose = "none"}, no progress information is printed.}

\item{b}{Optional input argument specifying the initial estimate of
the regression coefficients. It should be numeric vector of length
p.}

\item{prior.sd}{Optional input argument specifying the standard
deviations of the mixture components in the mixture-of-normals
prior.}

\item{prior.weights}{Optional initial estimate of the prior mixture
proportions. This vector will automatically be normalized so that
the entries represent proportions (that is, they add up to 1).}

\item{resid.sd}{Initial estimate of the residual standard deviation.}

\item{init.method}{Method used to initialize the estimates of the
regression coefficients.  When \code{init.method = "glmnet"}, the
estimates are initialized using
\code{\link[glmnet]{cv.glmnet}}. When \code{init.ethod = "null"},
the estimates are initialized to zero.}

\item{s}{The value of the glmnet penalty parameter at which the
coeffients are extracted (relevant for \code{init.method =
"glmnet"} only).}

\item{\dots}{Additional arguments passed to
\code{\link[glmnet]{cv.glmnet}} (relevant for \code{init.method =
"glmnet"} only).}
}
\value{
A list object with the following elements:

\item{intercept}{The estimated intercept.}

\item{b}{Posterior mean estimates of the regression coefficients.}

\item{resid.sd}{The estimated residual variance.}

\item{posterior.weights}{A vector of containing the estimated mixture
  proportions.}

\item{progress}{A list containing estimated parameters and objectives
over each outer-loop iteration.}

\item{update.order}{The ordering used for performing the
  coordinate-wise updates. For \code{update.order = "random"}, the
  orderings for outer-loop iterations are provided in a vector of
  length \code{p*max.iter}, where \code{p} is the number of predictors.}

\item{elbo}{Evidence lower bound of final fit.}

\item{phi}{A p x K matrix containing the posterior assignment
  probabilities, where p is the number of predictors, and K is the
  number of mixture components. (Each row of \code{phi} should sum to
  1.)}

\item{m}{A p x K matrix containing the posterior means conditional
  on assignment to each mixture component.}

\item{s2}{A p x K matrix containing the posterior variances
  conditional on assignment to each mixture component.}

\item{lfsr}{A vector of length p containing the local false
  discovery rate for each variable.}

\item{prior}{list of values used in prior}

\item{fitted}{fitted values for each row of \code{X}.}
}
\description{
Model fitting algorithms for Multiple Regression with
  Adaptive Shrinkage ("Mr.ASH"). Mr.ASH is a variational empirical
  Bayes (VEB) method for multiple linear regression. The fitting
 algorithms (locally) maximize the approximate marginal likelihood
  (the "evidence lower bound", or ELBO) via coordinate-wise updates.
}
\details{
Mr.ASH is a statistical inference method for the following
multiple linear regression model: \deqn{y | X, \beta, \sigma^2 ~
N(X \beta, \sigma I_n),} in which the regression coefficients
\eqn{\beta} admit a mixture-of-normals prior, \deqn{\beta | \pi,
\sigma ~ g = \sum_{k=1}^K N(0, \sigma^2 \sigma_k^2).} Each mixture
component in the prior, \eqn{g}, is a normal density centered at
zero, with variance \eqn{\sigma^2 \sigma_k^2}.

The fitting algorithm, if run for a large enough number of
iterations, will find an approximate posterior for the regression
coefficients, denoted by \eqn{q(\beta)}, residual variance
parameter \eqn{\sigma^2}, and prior mixture weights \eqn{\pi_1,
\ldots, \pi_K} maximizing the evidence lower bound, \deqn{F(q, \pi,
\sigma^2) = E_q log p(y | X, \beta, \sigma^2) - \sum_{j=1}^p
D_{KL}(q_j || g),} where \eqn{D_{KL}(q_j || g)} denotes the
Kullback-Leibler (KL) divergence, a measure of the "distance"
between (approximate) posterior \eqn{q_j(\beta_j)} and prior
\eqn{g(\beta_j)}. The fitting algorithm iteratively updates the
approximate posteriors \eqn{q_1, \ldots, q_p}, separately for each
\eqn{j = 1, \ldots, p} (in an order determined by
\code{update.order}), then separately updates the mixture weights
\eqn{\pi} and residual variance \eqn{\sigma^2}. This
coordinate-wise update scheme iterates until the convergence
criterion is met, or until the algorithm hits an upper bound on
the number of iterations (specified by \code{max.iter}).

See \sQuote{References} for more details about the model and
algorithm.

The \code{control} argument is a list in which any of the following
named components will override the default algorithm settings (as
defined by \code{mr_ash_control_default}):

\describe{

\item{\code{min.iter}}{The minimum number of outer loop iterations.}

\item{\code{max.iter}}{The maximum number of outer loop iterations.}

\item{\code{convtol}}{When \code{update.prior.weights = TRUE}, the
  outer-loop updates terminate when the largest change in the mixture
  weights is less than \code{convtol*K}; when \code{update.prior.weights =
  FALSE}, the outer-loop updates stop when the largest change in the
  estimates of the posterior mean coefficients is less than
  \code{convtol*K}.}

\item{\code{update.prior.weights}}{If \code{update.prior.weights = TRUE}, the mixture
  proportions in the mixture-of-normals prior are estimated from the
  data.}

\item{\code{update.resid.sd}}{If \code{update.resid.sd = TRUE}, the
  residual variance parameter \eqn{\sigma^2} is estimated from the
  data.}

\item{\code{update.order}}{The order in which the coordinate
  ascent updates for estimating the posterior mean coefficients are
  performed. \code{update.order} can be \code{NULL}, \code{"random"},
  or any permutation of \eqn{(1,\ldots,p)}, where \code{p} is the
  number of columns in the input matrix \code{X}. When
  \code{update.order} is \code{NULL}, the coordinate ascent updates
  are performed in order in which they appear in \code{X}; this is
  equivalent to setting \code{update.order = 1:p}. When
  \code{update.order = "random"}, the coordinate ascent updates are
  performed in a randomly generated order, and this random ordering
  is different at each outer-loop iteration.}

\item{\code{epstol}}{A small, positive number added to the
  likelihoods to avoid logarithms of zero.}}
}
\examples{
# Simulate a data set.
set.seed(1)
n <- 200
p <- 300
data <- simulate_regression_data(n, p, s = 250)

### fit Mr.ASH
fit.mr.ash <- fit_mr_ash(data$X,data$y)

### prediction routine
Xnew        = matrix(rnorm(n*p),n,p)
ynew        = Xnew \%*\% data$b + rnorm(n)
ypred       = predict(fit.mr.ash, Xnew)

### test error
rmse        = norm(ynew - ypred, '2') / sqrt(n)

### coefficients
bhat     = predict(fit.mr.ash, type = "coefficients")
# this equals c(fit.mr.ash$intercept, fit.mr.ash$b)

set.seed(1)
dat <- simulate_regression_data(n = 400, p = 100, s = 20)
X <- dat$X
y <- dat$y

# Initialize the coefficients using glmnet.
fit0_glmnet <- init_mr_ash(X, y)

# Initialize the coefficients to be all zero.
fit0_null <- init_mr_ash(X, y, init.method = "null")

# Randomly initialize the coefficients.
fit0_rand <- init_mr_ash(X, y, b = rnorm(100))

# Specify a custom mixture prior.
fit0_custom <- init_mr_ash(X, y, prior.sd = (2^((0:19)/20) - 1),
                           prior.weights = rep(1,20))

}
\references{
Y. Kim (2020), Bayesian shrinkage methods for high dimensional
regression. Ph.D. thesis, University of Chicago.
}
\seealso{
\code{\link{predict.mr.ash}}
}
