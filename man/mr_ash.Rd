% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/fit.R
\name{mr_ash}
\alias{mr_ash}
\alias{mr_ash_control_default}
\title{Multiple Regression with Adaptive Shrinkage}
\usage{
mr_ash(
  X,
  y,
  sa2 = NULL,
  beta.init = NULL,
  pi = NULL,
  sigma2 = NULL,
  standardize = FALSE,
  intercept = TRUE,
  control = list(),
  verbose = c("progress", "detailed", "none")
)

mr_ash_control_default()
}
\arguments{
\item{X}{The input matrix, of dimension (n,p); each column is a
single predictor; and each row is an observation vector. Here, n is
the number of samples and p is the number of predictors. The matrix
cannot be sparse.}

\item{y}{The observed continuously-valued responses, a vector of
length n.}

\item{sa2}{The vector of prior mixture component variances. The
variances should be in increasing order, starting at zero; that is,
\code{sort(sa2)} should be the same as \code{sa2}. When \code{sa2}
is \code{NULL}, the default setting is used, \code{sa2[k] =
(2^(0.05*(k-1)) - 1)^2}, for \code{k = 1:20}. For this default
setting, \code{sa2[1] = 0}, and \code{sa2[20]} is roughly 1.}

\item{beta.init}{The initial estimate of the (approximate)
posterior mean regression coefficients. This should be \code{NULL},
or a vector of length p. When \code{beta.init} is \code{NULL}, the
posterior mean coefficients are all initially set to zero.}

\item{pi}{The initial estimate of the mixture proportions
\eqn{\pi_1, \ldots, \pi_K}. If \code{pi} is \code{NULL}, the
mixture weights are initialized to \code{rep(1/K,K)}, where
\code{K = length(sa2)}.}

\item{sigma2}{The initial estimate of the residual variance,
\eqn{\sigma^2}. If \code{sigma2 = NULL}, the residual variance is
initialized to the empirical variance of the residuals based on the
initial estimates of the regression coefficients, \code{beta.init},
after removing linear effects of the intercept and any covariances.}

\item{standardize}{The logical flag for standardization of the
columns of X variable, prior to the model fitting. The coefficients
are always returned on the original scale.}

\item{intercept}{When \code{intercept = TRUE}, an intercept is
included in the regression model.}

\item{control}{A list of parameters controlling the behaviour of
the optimization algorithm. See \sQuote{Details}.}

\item{verbose}{When \code{verbose = "detailed"}, detailed
information about the algorithm's progress is printed to the
console at each iteration; when \code{verbose = "progressbar"}, a
plus (\dQuote{+}) is printed for each outer-loop iteration; and
when \code{verbose = "none"}, no progress information is printed.}
}
\value{
A list object with the following elements:

\item{intercept}{The estimated intercept.}

\item{beta}{Posterior mean estimates of the regression coefficients.}

\item{sigma2}{The estimated residual variance.}

\item{pi}{A vector of containing the estimated mixture
  proportions.}

\item{progress}{A list containing estimated parameters and objectives
over each outer-loop iteration.}

\item{update.order}{The ordering used for performing the
  coordinate-wise updates. For \code{update.order = "random"}, the
  orderings for outer-loop iterations are provided in a vector of
  length \code{p*max.iter}, where \code{p} is the number of predictors.}

\item{elbo}{Evidence lower bound of final fit.}

\item{phi}{A p x K matrix containing the posterior assignment
  probabilities, where p is the number of predictors, and K is the
  number of mixture components. (Each row of \code{phi} should sum to
  1.)}

\item{m}{A p x K matrix containing the posterior means conditional
  on assignment to each mixture component.}

\item{s2}{A p x K matrix containing the posterior variances
  conditional on assignment to each mixture component.}

\item{lfsr}{A vector of length p containing the local false
  discovery rate for each variable.}

\item{sa2}{vector of prior mixture component variances.}

\item{fitted}{fitted values for each row of \code{X}.}
}
\description{
Model fitting algorithms for Multiple Regression with
  Adaptive Shrinkage ("Mr.ASH"). Mr.ASH is a variational empirical
  Bayes (VEB) method for multiple linear regression. The fitting
 algorithms (locally) maximize the approximate marginal likelihood
  (the "evidence lower bound", or ELBO) via coordinate-wise updates.
}
\details{
Mr.ASH is a statistical inference method for the following
multiple linear regression model: \deqn{y | X, \beta, \sigma^2 ~
N(X \beta, \sigma I_n),} in which the regression coefficients
\eqn{\beta} admit a mixture-of-normals prior, \deqn{\beta | \pi,
\sigma ~ g = \sum_{k=1}^K N(0, \sigma^2 \sigma_k^2).} Each mixture
component in the prior, \eqn{g}, is a normal density centered at
zero, with variance \eqn{\sigma^2 \sigma_k^2}.

The fitting algorithm, if run for a large enough number of
iterations, will find an approximate posterior for the regression
coefficients, denoted by \eqn{q(\beta)}, residual variance
parameter \eqn{\sigma^2}, and prior mixture weights \eqn{\pi_1,
\ldots, \pi_K} maximizing the evidence lower bound, \deqn{F(q, \pi,
\sigma^2) = E_q log p(y | X, \beta, \sigma^2) - \sum_{j=1}^p
D_{KL}(q_j || g),} where \eqn{D_{KL}(q_j || g)} denotes the
Kullback-Leibler (KL) divergence, a measure of the "distance"
between (approximate) posterior \eqn{q_j(\beta_j)} and prior
\eqn{g(\beta_j)}. The fitting algorithm iteratively updates the
approximate posteriors \eqn{q_1, \ldots, q_p}, separately for each
\eqn{j = 1, \ldots, p} (in an order determined by
\code{update.order}), then separately updates the mixture weights
\eqn{\pi} and residual variance \eqn{\sigma^2}. This
coordinate-wise update scheme iterates until the convergence
criterion is met, or until the algorithm hits an upper bound on
the number of iterations (specified by \code{max.iter}).

See \sQuote{References} for more details about the model and
algorithm.

The \code{control} argument is a list in which any of the following
named components will override the default algorithm settings (as
defined by \code{mr_ash_control_default}):

\describe{

\item{\code{min.iter}}{The minimum number of outer loop iterations.}

\item{\code{max.iter}}{The maximum number of outer loop iterations.}

\item{\code{convtol}}{When \code{update.pi = TRUE}, the
  outer-loop updates terminate when the largest change in the mixture
  weights is less than \code{convtol*K}; when \code{update.pi =
  FALSE}, the outer-loop updates stop when the largest change in the
  estimates of the posterior mean coefficients is less than
  \code{convtol*K}.}

\item{\code{update.pi}}{If \code{update.pi = TRUE}, the mixture
  proportions in the mixture-of-normals prior are estimated from the
  data.}

\item{\code{update.sigma2}}{If \code{update.sigma2 = TRUE}, the
  residual variance parameter \eqn{\sigma^2} is estimated from the
  data.}

\item{\code{update.order}}{The order in which the coordinate
  ascent updates for estimating the posterior mean coefficients are
  performed. \code{update.order} can be \code{NULL}, \code{"random"},
  or any permutation of \eqn{(1,\ldots,p)}, where \code{p} is the
  number of columns in the input matrix \code{X}. When
  \code{update.order} is \code{NULL}, the coordinate ascent updates
  are performed in order in which they appear in \code{X}; this is
  equivalent to setting \code{update.order = 1:p}. When
  \code{update.order = "random"}, the coordinate ascent updates are
  performed in a randomly generated order, and this random ordering
  is different at each outer-loop iteration.}

\item{\code{epstol}}{A small, positive number added to the
  likelihoods to avoid logarithms of zero.}}
}
\examples{
# Simulate a data set.
set.seed(1)
n          <- 200
p          <- 300
X          <- matrix(rnorm(n*p),n,p)
beta       <- double(p)
beta[1:10] <- 1:10
y          <- drop(X \%*\% beta + rnorm(n))

### fit Mr.ASH
fit.mr.ash <- mr_ash(X,y)

### prediction routine
Xnew        = matrix(rnorm(n*p),n,p)
ynew        = Xnew \%*\% beta + rnorm(n)
ypred       = predict(fit.mr.ash, Xnew)

### test error
rmse        = norm(ynew - ypred, '2') / sqrt(n)

### coefficients
betahat     = predict(fit.mr.ash, type = "coefficients")
# this equals c(fit.mr.ash$intercept, fit.mr.ash$beta)

}
\references{
Y. Kim (2020), Bayesian shrinkage methods for high dimensional
regression. Ph.D. thesis, University of Chicago.
}
\seealso{
\code{\link{predict.mr.ash}}
}
